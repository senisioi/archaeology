# Archaeology Labs

- Lab 1 - [Introduction to stylometry](https://github.com/senisioi/archaeology/blob/main/labs/L1.md); original material [published here](https://doi.org/10.46430/phen0078); we will use spaCy instead of NLTK and take a slightly different approach

- Lab 2 - [n-gram Language Models](https://github.com/senisioi/archaeology/blob/main/labs/L2.md), we will explore the probabilistic definition of language models and link them to the story of [A.A. Markov](https://www.americanscientist.org/article/first-links-in-the-markov-chain), [Claude Shannon](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf), [Noam Chomsky](https://chomsky.info/wp-content/uploads/195609-.pdf), and [Solomon Marcus](http://dspace.bcu-iasi.ro/handle/123456789/2760)

- Lab 3 - vector representation of words, [colab here](https://colab.research.google.com/drive/10FDqqXDQMhDZVdrHmclDBk3bSu8uNxne?usp=sharing)

- Lab4 - recurrent neural networks, follow the [pre-requisite reading here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) and the tiny [history section from wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network#History) then 1. run the [pytorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) to do classification with RNNs, 2. solve the [exercises at the end](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#exercises) 3. run the pytorch tutorial to [generate text with RNNs](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) 

- Lab5 - [transformers](https://colab.research.google.com/drive/1B0rS1czKhr5pLZY2Y5Wy6GpGOno28on0?usp=sharing)